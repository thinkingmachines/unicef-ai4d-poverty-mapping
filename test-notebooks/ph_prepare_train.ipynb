{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Prepare Model Training Data (Philippines)\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> This notebook prepare the data for the poverty mapping model for the Philippines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the configuration for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_config = dict(\n",
    "    save_path=\"../data/outputs/\",\n",
    "    repo_path=\"../data/SVII_PH_KH_MM_TL\",\n",
    "    download_gcs_uri=\"gs://poverty-mapping/outputs/\",\n",
    "    output_gcs_uri=\"gs://poverty-mapping/outputs/\",\n",
    "    data_dir=\"ph\",\n",
    "    country=\"ph\",\n",
    "    ookla_folder=\"ookla_ph\",\n",
    "    hdx_folder=\"hdx_ph\",\n",
    "    dhs_folder=\"dhs_ph\",\n",
    "    osm_folder=\"osm_ph\",\n",
    "    dhs_geo_zip_folder=\"PHGE71FL\",\n",
    "    dhs_zip_folder=\"PHHR71DT\",\n",
    "    training_folder=\"training_ph\",\n",
    "    viirs_folder=\"viirs_ph\",\n",
    "    # crs=\"4683\",\n",
    "    # ookla_feature=\"avg_d_mbps\",\n",
    "    # boundary_file=\"phl_adminboundaries_candidate_adm3\",\n",
    "    year=\"2020\",\n",
    "    quarter=\"2\",\n",
    "    sample=False,\n",
    "    random_sample=False,\n",
    "    no_samples=60,\n",
    "    random_seed=42,\n",
    "    # clust_rad=2000,\n",
    "    # plot_ookla_features=True,\n",
    "    # adm_level=3,\n",
    "    # use_pcode=True,\n",
    "    # shape_label=\"ADM3_PCODE\",\n",
    "    # bins=6,\n",
    "    # show_legend=False,\n",
    "    use_ookla=True,\n",
    "    use_viirs=True,\n",
    "    use_osm=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the different datasets from the cloud storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping existing item: file://../data/outputs/ookla_ph/PHGE71FL_cluster_coords.csv\n",
      "Skipping existing item: file://../data/outputs/ookla_ph/ph_2020_2_avg_d_mbps.csv\n",
      "Skipping existing item: file://../data/outputs/ookla_ph/ph_2020_2_avg_d_mbps_by_pcode_adm3.geojson\n",
      "Skipping existing item: file://../data/outputs/ookla_ph/phl_adminboundaries_candidate_adm3_avg_d_mbps.jpeg\n",
      "Skipping existing item: file://../data/outputs/dhs_ph/PHGE71FL_cluster_coords.csv\n",
      "Skipping existing item: file://../data/outputs/dhs_ph/PHHR71DT_PHGE71FL_by_cluster.csv\n",
      "Skipping existing item: file://../data/outputs/dhs_ph/PHHR71DT_PHGE71FL_by_cluster.geojson\n",
      "Skipping existing item: file://../data/outputs/dhs_ph/PHHR71DT_base.csv\n",
      "Skipping existing item: file://../data/outputs/dhs_ph/PHHR71DT_raw.csv\n",
      "Skipping existing item: file://../data/outputs/viirs_ph/ph_2017_viirs_avg_rad_zonal_stats.csv\n",
      "Skipping existing item: file://../data/outputs/viirs_ph/ph_2017_viirs_avg_rad_zonal_stats.gpkg\n",
      "Skipping existing item: file://../data/outputs/viirs_ph/qgis_viirs/qgis_rzs_dhs_viirs_ph_2017.geojson\n",
      "Skipping existing item: file://../data/outputs/viirs_ph/qgis_viirs/qgis_rzs_dhs_viirs_ph_2017.gpkg\n",
      "Skipping existing item: file://../data/outputs/osm_ph/PHGE71FL_cluster_coords.csv\n",
      "Skipping existing item: file://../data/outputs/osm_ph/PHGE71FL_cluster_coords_osm_agg.csv\n",
      "Skipping existing item: file://../data/outputs/osm_ph/osm_ph/PHGE71FL_cluster_coords.csv\n",
      "Skipping existing item: file://../data/outputs/osm_ph/osm_ph/PHGE71FL_cluster_coords_osm_agg.csv\n",
      "Copying gs://poverty-mapping/outputs/training_ph/training_ph/data_labels.csv...\n",
      "Copying gs://poverty-mapping/outputs/training_ph/data_labels.csv...\n",
      "- [2/2 files][881.6 KiB/881.6 KiB] 100% Done                                    \n",
      "Operation completed over 2 objects/881.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "dataset_folder_keys = [\"ookla_folder\", \"dhs_folder\", \"viirs_folder\", \"osm_folder\", \"training_folder\"]\n",
    "\n",
    "for key in dataset_folder_keys:\n",
    "    gcs_download_folder = prepare_config['download_gcs_uri'] +  prepare_config[key] \n",
    "    save_path = prepare_config['save_path']\n",
    "    subprocess.call([f'gsutil -m cp -n -r {gcs_download_folder} {save_path}'], shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filepaths\n",
    "## hardcoded for now\n",
    "## TODO: Integrate into config once working\n",
    "cluster_coords_labels_filepath = \"../data/outputs/ph/dhs_ph/PHGE71FL_cluster_coords.csv\"\n",
    "ookla_filepath = '../data/outputs/ph/ookla_ph/ph_2020_2_avg_d_mbps.csv'\n",
    "viirs_filepath = '../data/outputs/ph/viirs_ph/ph_2017_viirs_avg_rad_zonal_stats.csv'\n",
    "osm_filepath = '../data/outputs/ph/osm_ph/PHGE71FL_cluster_coords_osm_agg.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config flags\n",
    "## hardcoded for now\n",
    "## TODO: Integrate into config once working\n",
    "\n",
    "sample=True\n",
    "no_samples=60\n",
    "random_sample=False\n",
    "random_seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1250 entries, 0 to 1249\n",
      "Data columns (total 65 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   DHSID                   1250 non-null   object \n",
      " 1   DHSCC                   1250 non-null   object \n",
      " 2   DHSYEAR                 1250 non-null   float64\n",
      " 3   DHSCLUST                1250 non-null   float64\n",
      " 4   CCFIPS                  0 non-null      float64\n",
      " 5   ADM1FIPS                0 non-null      float64\n",
      " 6   ADM1FIPSNA              0 non-null      float64\n",
      " 7   ADM1SALBNA              0 non-null      float64\n",
      " 8   ADM1SALBCO              0 non-null      float64\n",
      " 9   ADM1DHS                 1250 non-null   float64\n",
      " 10  ADM1NAME                1250 non-null   object \n",
      " 11  DHSREGCO                1250 non-null   float64\n",
      " 12  DHSREGNA                1250 non-null   object \n",
      " 13  SOURCE                  1250 non-null   object \n",
      " 14  URBAN_RURA              1250 non-null   object \n",
      " 15  LATNUM                  1250 non-null   float64\n",
      " 16  LONGNUM                 1250 non-null   float64\n",
      " 17  ALT_GPS                 1250 non-null   float64\n",
      " 18  ALT_DEM                 1250 non-null   float64\n",
      " 19  DATUM                   1250 non-null   object \n",
      " 20  geometry                1250 non-null   object \n",
      " 21  avg_d_mbps              988 non-null    float64\n",
      " 22  avg_rad_min             1178 non-null   float64\n",
      " 23  avg_rad_max             1178 non-null   float64\n",
      " 24  avg_rad_mean            1178 non-null   float64\n",
      " 25  avg_rad_std             1178 non-null   float64\n",
      " 26  avg_rad_median          1178 non-null   float64\n",
      " 27  building_ct             1250 non-null   int64  \n",
      " 28  no_roads                1250 non-null   int64  \n",
      " 29  no_primary_roads        1250 non-null   int64  \n",
      " 30  no_trunk_roads          1250 non-null   int64  \n",
      " 31  marketplace_count       1250 non-null   int64  \n",
      " 32  charging_station_count  1250 non-null   int64  \n",
      " 33  post_box_count          1250 non-null   int64  \n",
      " 34  post_office_count       1250 non-null   int64  \n",
      " 35  pharmacy_count          1250 non-null   int64  \n",
      " 36  hospital_count          1250 non-null   int64  \n",
      " 37  dentist_count           1250 non-null   int64  \n",
      " 38  restaurant_count        1250 non-null   int64  \n",
      " 39  food_court_count        1250 non-null   int64  \n",
      " 40  cafe_count              1250 non-null   int64  \n",
      " 41  fast_food_count         1250 non-null   int64  \n",
      " 42  police_count            1250 non-null   int64  \n",
      " 43  townhall_count          1250 non-null   int64  \n",
      " 44  fire_station_count      1250 non-null   int64  \n",
      " 45  social_facility_count   1250 non-null   int64  \n",
      " 46  courthouse_count        1250 non-null   int64  \n",
      " 47  fuel_count              1250 non-null   int64  \n",
      " 48  bus_station_count       1250 non-null   int64  \n",
      " 49  bank_count              1250 non-null   int64  \n",
      " 50  atm_count               1250 non-null   int64  \n",
      " 51  library_count           1250 non-null   int64  \n",
      " 52  helipad_count           1250 non-null   int64  \n",
      " 53  aerodrome_count         1250 non-null   int64  \n",
      " 54  subway_entrance_count   1250 non-null   int64  \n",
      " 55  hotel_count             1250 non-null   int64  \n",
      " 56  camp_site_count         1250 non-null   int64  \n",
      " 57  city_count              1250 non-null   int64  \n",
      " 58  convenience_count       1250 non-null   int64  \n",
      " 59  supermarket_count       1250 non-null   int64  \n",
      " 60  car_repair_count        1250 non-null   int64  \n",
      " 61  department_store_count  1250 non-null   int64  \n",
      " 62  computer_count          1250 non-null   int64  \n",
      " 63  playground_count        1250 non-null   int64  \n",
      " 64  monument_count          1250 non-null   int64  \n",
      "dtypes: float64(19), int64(38), object(8)\n",
      "memory usage: 644.5+ KB\n"
     ]
    }
   ],
   "source": [
    "save_path = prepare_config['save_path']\n",
    "\n",
    "# Load the cluster centroid df\n",
    "cluster_centroid_df = pd.read_csv(\n",
    "    os.path.join(cluster_coords_labels_filepath)\n",
    ")\n",
    "\n",
    "\n",
    "# sample clusters\n",
    "if prepare_config[\"sample\"]:\n",
    "    no_samples = prepare_config[\"no_samples\"]\n",
    "    seed = prepare_config[\"random_seed\"]\n",
    "    if prepare_config[\"random_sample\"]:\n",
    "        cluster_centroid_df = cluster_centroid_df.sample(\n",
    "            no_samples, random_state=seed\n",
    "        )\n",
    "    else:\n",
    "        cluster_centroid_df = cluster_centroid_df.head(no_samples)\n",
    "\n",
    "\n",
    "\n",
    "# dataframes to add\n",
    "data_frames = []\n",
    "# first add our cluster df\n",
    "data_frames.append(cluster_centroid_df)\n",
    "\n",
    "# ookla\n",
    "if prepare_config[\"use_ookla\"]:\n",
    "\n",
    "    ## NOTE: Commented out these code cells for building the filepaths\n",
    "    ## to save time; opting to hardcode instead \n",
    "    ## TODO: Redo these cells when we have figured out the desired folder\n",
    "    ## structure\n",
    "\n",
    "    # ookla_feature = \"avg_d_mbps\"\n",
    "    # country = prepare_config[\"country\"]\n",
    "    # year = prepare_config[\"year\"]\n",
    "    # quarter = prepare_config[\"quarter\"]\n",
    "    # # # from repo dir\n",
    "    # # result_file_path = os.path.join(\n",
    "    # #     save_path, f\"{country}_{year}_{quarter}_{ookla_feature}.csv\"\n",
    "    # # )\n",
    "    # # from guild run output\n",
    "    # result_file_path = f\"{country}_{year}_{quarter}_{ookla_feature}.csv\"\n",
    "\n",
    "    mean_download_by_cluster = pd.read_csv(ookla_filepath)\n",
    "    # append\n",
    "    data_frames.append(mean_download_by_cluster)\n",
    "\n",
    "# viirs\n",
    "if prepare_config[\"use_viirs\"]:\n",
    "\n",
    "    ## NOTE: Commented out these code cells for building the filepaths\n",
    "    ## to save time; opting to hardcode instead \n",
    "    ## TODO: Redo these cells when we have figured out the desired folder\n",
    "    ## structure\n",
    "    # satellite_result_file_name = f\"{cluster_coords_filename}_gee_agg.csv\"\n",
    "\n",
    "    # # from repo dir\n",
    "    # satellite_result_save_path = os.path.join(save_path, satellite_result_file_name)\n",
    "    # from guild run\n",
    "    # satellite_result_save_path = satellite_result_file_name\n",
    "\n",
    "    viirs_result = pd.read_csv(viirs_filepath)\n",
    "    data_frames.append(viirs_result)\n",
    "\n",
    "# # osm\n",
    "if prepare_config[\"use_osm\"]:\n",
    "    ## NOTE: Commented out these code cells for building the filepaths\n",
    "    ## to save time; opting to hardcode instead \n",
    "    ## TODO: Redo these cells when we have figured out the desired folder\n",
    "    ## structure\n",
    "\n",
    "    # osm_result_file_name = f\"{cluster_coords_filename}_osm_agg.csv\"\n",
    "    # # from repo dir\n",
    "    # osm_result_save_path = os.path.join(save_path, osm_result_file_name)\n",
    "    # from guild run\n",
    "    # osm_result_save_path = osm_result_file_name\n",
    "\n",
    "    osm_result = pd.read_csv(osm_filepath)\n",
    "    data_frames.append(osm_result)\n",
    "\n",
    "\n",
    "df_merged = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"DHSID\"], how=\"outer\"),\n",
    "    data_frames,\n",
    ")\n",
    "\n",
    "# filter out redundant columns\n",
    "# TODO: find edge cases and change comprehension below accordingly\n",
    "relev_columns = [col for col in df_merged.columns if col.split(\"_\")[-1] not in \"xy\"]\n",
    "df_merged = df_merged[relev_columns]\n",
    "\n",
    "unnamed_col_labels = [col for col in df_merged.columns if \"Unnamed\" in col]\n",
    "df_merged = df_merged.drop(unnamed_col_labels, axis=1)\n",
    "\n",
    "data_labels_filepath = os.path.join(save_path, \"data_labels.csv\") \n",
    "df_merged.to_csv(data_labels_filepath)\n",
    "\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../data/outputs/ph/data_labels.csv [Content-Type=text/csv]...\n",
      "- [1 files][421.9 KiB/421.9 KiB]                                                \n",
      "Operation completed over 1 objects/421.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp {data_labels_filepath} {prepare_config['download_gcs_uri'] + 'training_ph/'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9c652687746b7e37dd5eba4a53c758a3d21cd9cb60962b6bb30d0de25d1ae14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
